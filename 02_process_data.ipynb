{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing (02)\n",
    "\n",
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sara import eda, eda_unique\n",
    "import nltk as nltk\n",
    "\n",
    "# model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nlp\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "# stop future warnings\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Cleaned Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_cleaned = pd.read_csv('data/reddit_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Nulls (Again)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_cleaned.dropna(subset=['title'],inplace=True)\n",
    "df_reddit_cleaned.dropna(subset=['selftext'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review Top N Words in Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# tokenize title and self text\n",
    "df_reddit_cleaned['title_tokens'] = df_reddit_cleaned['title'].apply(tokenizer.tokenize)\n",
    "df_reddit_cleaned['selftext_tokens'] = df_reddit_cleaned['selftext'].apply(tokenizer.tokenize)\n",
    "\n",
    "# create vars for title words, title lengths, and title words (unique)\n",
    "title_words = [word for tokens in df_reddit_cleaned['title_tokens'] for word in tokens]\n",
    "title_lengths = [len(tokens) for tokens in df_reddit_cleaned['title_tokens']]\n",
    "title_vocab = sorted(list(set(title_words)))\n",
    "\n",
    "# create vars for selftext words, selftext lengths, and selftext words (unique)\n",
    "selftext_words = [word for tokens in df_reddit_cleaned['selftext_tokens'] for word in tokens]\n",
    "selftext_lengths = [len(tokens) for tokens in df_reddit_cleaned['selftext_tokens']]\n",
    "selftext_vocab = sorted(list(set(selftext_words)))\n",
    "\n",
    "# get top word frequency fro m the variables (titles and selftexts)\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# create masks for each subreddit\n",
    "mask_cooking = df_reddit_cleaned[df_reddit_cleaned['subreddit']=='Cooking']\n",
    "mask_baking = df_reddit_cleaned[df_reddit_cleaned['subreddit']=='Baking']\n",
    "\n",
    "# get top 20 words for all posts, cooking posts, and baking posts\n",
    "all_top_20 = get_top_n_words(df_reddit_cleaned['title'],20)\n",
    "cooking_top_20 = get_top_n_words(mask_cooking['title'],20)\n",
    "baking_top_20 = get_top_n_words(mask_baking['title'],20)\n",
    "\n",
    "# https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide on Additional Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stops = ['recipe','cooking','make',\n",
    "             'help','chicken','recipes',\n",
    "             'cook','best','good',\n",
    "             'cake','need','use',\n",
    "             'food','question','sauce',\n",
    "             'making','looking','ideas',\n",
    "             'anyone','made','http','https','remove']\n",
    "\n",
    "for i in new_stops:\n",
    "    allstopwords.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_processed(doc_raw):\n",
    "    \"\"\"Input raw doc from reddit post, output proccessed doc.\"\"\"\n",
    "    \n",
    "    # Instantiate BeautifulSoup and convert doc_raw to doc_text\n",
    "    doc_text = BeautifulSoup(doc_raw).get_text()\n",
    "    \n",
    "    # Remove non-letters\n",
    "    doc_letters = re.sub('[^a-zA-Z]',' ',doc_text)\n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    doc_words = doc_letters.lower().split()\n",
    "    \n",
    "    # Convert stopwords to a set\n",
    "    stops = set(allstopwords)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    doc_meaningful_words = [w for w in doc_words if not w in stops]\n",
    "    \n",
    "    # Join words back into one string\n",
    "    doc_processed = (' '.join(doc_meaningful_words))    \n",
    "    \n",
    "    # Return processed titles\n",
    "    return doc_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Titles\n",
    "df_reddit_cleaned['title'] = df_reddit_cleaned['title'].apply(doc_processed)\n",
    "\n",
    "# Process Selftext\n",
    "df_reddit_cleaned['selftext'] = df_reddit_cleaned['selftext'].apply(doc_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Subreddit Column to Binary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "110373    0\n",
       "110374    0\n",
       "110375    0\n",
       "110376    0\n",
       "110377    0\n",
       "Name: subreddit, Length: 110378, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit_cleaned['subreddit'] = df_reddit_cleaned['subreddit'].map({'Cooking':1,'Baking':0})\n",
    "df_reddit_cleaned['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Processed Data into CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_cleaned.to_csv('data/reddit_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "- Tried Lemma\n",
    "- Tried Porterstem\n",
    "- Tried Stemming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
